/afs/inf.ed.ac.uk/user/s17/s1738075/.local/lib/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
/afs/inf.ed.ac.uk/user/s17/s1738075/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
early_fusion_cnn.py:139: RuntimeWarning: invalid value encountered in divide
  train_set_audio = train_set_audio / audio_max
early_fusion_cnn.py:141: RuntimeWarning: invalid value encountered in divide
  test_set_audio = test_set_audio / audio_max
2018-03-20 23:20:47.480070: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
This API will be deprecated in the future versions. Please check the Github page for the current API

Modality facet for video 8qrpnFRGt2A segment 13 is (partially) missing and is thus being replaced by zeros!

Train on 1283 samples, validate on 229 samples
Epoch 1/100

  64/1283 [>.............................] - ETA: 6s - loss: 0.7068 - acc: 0.4219
 256/1283 [====>.........................] - ETA: 1s - loss: 0.6929 - acc: 0.4961
 448/1283 [=========>....................] - ETA: 0s - loss: 0.6925 - acc: 0.5000
 640/1283 [=============>................] - ETA: 0s - loss: 0.6883 - acc: 0.5094
 832/1283 [==================>...........] - ETA: 0s - loss: 0.6934 - acc: 0.5204
1024/1283 [======================>.......] - ETA: 0s - loss: 0.6889 - acc: 0.5322
1216/1283 [===========================>..] - ETA: 0s - loss: 0.6866 - acc: 0.5387
1283/1283 [==============================] - 1s 657us/step - loss: 0.6845 - acc: 0.5456 - val_loss: 0.6917 - val_acc: 0.5371

Epoch 00001: val_acc improved from -inf to 0.53712, saving model to classification_logs//cnn_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.1_nl_2_ml_20/saved_models/best_validation_cnn_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.1_nl_2_ml_20.ckpt
Epoch 2/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6620 - acc: 0.5625
 256/1283 [====>.........................] - ETA: 0s - loss: 0.6652 - acc: 0.5781
 448/1283 [=========>....................] - ETA: 0s - loss: 0.6624 - acc: 0.5848
 640/1283 [=============>................] - ETA: 0s - loss: 0.6786 - acc: 0.5797
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6677 - acc: 0.5949
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6636 - acc: 0.5998
1283/1283 [==============================] - 0s 311us/step - loss: 0.6590 - acc: 0.6072 - val_loss: 0.6971 - val_acc: 0.5371

Epoch 00002: val_acc improved from 0.53712 to 0.53712, saving model to classification_logs//cnn_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.1_nl_2_ml_20/saved_models/best_validation_cnn_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.1_nl_2_ml_20.ckpt
Epoch 3/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6685 - acc: 0.6250
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6478 - acc: 0.6062
 576/1283 [============>.................] - ETA: 0s - loss: 0.6456 - acc: 0.6198
 832/1283 [==================>...........] - ETA: 0s - loss: 0.6406 - acc: 0.6346
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6511 - acc: 0.6268
1280/1283 [============================>.] - ETA: 0s - loss: 0.6506 - acc: 0.6258
1283/1283 [==============================] - 0s 260us/step - loss: 0.6505 - acc: 0.6267 - val_loss: 0.7115 - val_acc: 0.5022

Epoch 00003: val_acc did not improve
Epoch 4/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6459 - acc: 0.5156
 256/1283 [====>.........................] - ETA: 0s - loss: 0.6209 - acc: 0.6367
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6208 - acc: 0.6484
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6296 - acc: 0.6392
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6287 - acc: 0.6473
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6226 - acc: 0.6519
1283/1283 [==============================] - 0s 307us/step - loss: 0.6214 - acc: 0.6500 - val_loss: 0.7218 - val_acc: 0.5371

Epoch 00004: val_acc did not improve
Epoch 5/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6054 - acc: 0.6406
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6087 - acc: 0.6469
 576/1283 [============>.................] - ETA: 0s - loss: 0.6001 - acc: 0.6580
 768/1283 [================>.............] - ETA: 0s - loss: 0.6031 - acc: 0.6641
1024/1283 [======================>.......] - ETA: 0s - loss: 0.6111 - acc: 0.6582
1283/1283 [==============================] - 0s 262us/step - loss: 0.6162 - acc: 0.6602 - val_loss: 0.7171 - val_acc: 0.5022

Epoch 00005: val_acc did not improve
Epoch 6/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5541 - acc: 0.7344
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5898 - acc: 0.6844
 576/1283 [============>.................] - ETA: 0s - loss: 0.5810 - acc: 0.6910
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5901 - acc: 0.6839
1088/1283 [========================>.....] - ETA: 0s - loss: 0.5952 - acc: 0.6710
1280/1283 [============================>.] - ETA: 0s - loss: 0.5890 - acc: 0.6750
1283/1283 [==============================] - 0s 259us/step - loss: 0.5890 - acc: 0.6750 - val_loss: 0.7584 - val_acc: 0.5284

Epoch 00006: val_acc did not improve
Epoch 7/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5644 - acc: 0.7188
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5720 - acc: 0.7031
 512/1283 [==========>...................] - ETA: 0s - loss: 0.5686 - acc: 0.6855
 704/1283 [===============>..............] - ETA: 0s - loss: 0.5738 - acc: 0.6875
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5693 - acc: 0.7000
1216/1283 [===========================>..] - ETA: 0s - loss: 0.5780 - acc: 0.6998
1283/1283 [==============================] - 0s 265us/step - loss: 0.5771 - acc: 0.7007 - val_loss: 0.7880 - val_acc: 0.5022

Epoch 00007: val_acc did not improve
Epoch 8/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5865 - acc: 0.6406
 256/1283 [====>.........................] - ETA: 0s - loss: 0.5367 - acc: 0.7227
 448/1283 [=========>....................] - ETA: 0s - loss: 0.5505 - acc: 0.7188
 576/1283 [============>.................] - ETA: 0s - loss: 0.5590 - acc: 0.7135
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5618 - acc: 0.7031
1024/1283 [======================>.......] - ETA: 0s - loss: 0.5614 - acc: 0.7051
1283/1283 [==============================] - 0s 295us/step - loss: 0.5569 - acc: 0.7178 - val_loss: 0.7604 - val_acc: 0.5022

Epoch 00008: val_acc did not improve
Epoch 9/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5278 - acc: 0.7656
 256/1283 [====>.........................] - ETA: 0s - loss: 0.5519 - acc: 0.7305
 512/1283 [==========>...................] - ETA: 0s - loss: 0.5459 - acc: 0.7305
 768/1283 [================>.............] - ETA: 0s - loss: 0.5390 - acc: 0.7214
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5263 - acc: 0.7333
1216/1283 [===========================>..] - ETA: 0s - loss: 0.5268 - acc: 0.7303
1283/1283 [==============================] - 0s 268us/step - loss: 0.5271 - acc: 0.7311 - val_loss: 0.8889 - val_acc: 0.5109

Epoch 00009: val_acc did not improve
Epoch 10/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.4231 - acc: 0.7812
 256/1283 [====>.........................] - ETA: 0s - loss: 0.4687 - acc: 0.7812
 448/1283 [=========>....................] - ETA: 0s - loss: 0.4970 - acc: 0.7500
 640/1283 [=============>................] - ETA: 0s - loss: 0.5011 - acc: 0.7516
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5045 - acc: 0.7464
1024/1283 [======================>.......] - ETA: 0s - loss: 0.5238 - acc: 0.7373
1280/1283 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.7406
1283/1283 [==============================] - 0s 315us/step - loss: 0.5156 - acc: 0.7412 - val_loss: 0.7585 - val_acc: 0.4978

Epoch 00010: val_acc did not improve
Epoch 11/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.4307 - acc: 0.7969
 256/1283 [====>.........................] - ETA: 0s - loss: 0.4502 - acc: 0.7812
 576/1283 [============>.................] - ETA: 0s - loss: 0.4682 - acc: 0.7691
 832/1283 [==================>...........] - ETA: 0s - loss: 0.4836 - acc: 0.7572
1152/1283 [=========================>....] - ETA: 0s - loss: 0.4796 - acc: 0.7552
1283/1283 [==============================] - 0s 233us/step - loss: 0.4842 - acc: 0.7514 - val_loss: 0.8269 - val_acc: 0.5022

Epoch 00011: val_acc did not improve
Epoch 12/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.4317 - acc: 0.8281
 320/1283 [======>.......................] - ETA: 0s - loss: 0.4339 - acc: 0.8125
 512/1283 [==========>...................] - ETA: 0s - loss: 0.4322 - acc: 0.8125
 768/1283 [================>.............] - ETA: 0s - loss: 0.4402 - acc: 0.8021
 960/1283 [=====================>........] - ETA: 0s - loss: 0.4381 - acc: 0.8000
1152/1283 [=========================>....] - ETA: 0s - loss: 0.4417 - acc: 0.7917
1283/1283 [==============================] - 0s 283us/step - loss: 0.4385 - acc: 0.7935 - val_loss: 0.8786 - val_acc: 0.5328

Epoch 00012: val_acc did not improve
Epoch 00012: early stopping
batch_size=64
batch_norm=False
dropout_rate=0.1
n_layers=2
max_len=20
epochs=100
mode=V
accuracy=0.44314868804664725
best_valid_accuracy=0.4970845481049563
