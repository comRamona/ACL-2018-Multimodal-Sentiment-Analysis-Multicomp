/afs/inf.ed.ac.uk/user/s17/s1738075/.local/lib/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
/afs/inf.ed.ac.uk/user/s17/s1738075/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
early_fusion_lstm.py:139: RuntimeWarning: invalid value encountered in divide
  train_set_audio = train_set_audio / audio_max
early_fusion_lstm.py:141: RuntimeWarning: invalid value encountered in divide
  test_set_audio = test_set_audio / audio_max
2018-03-20 09:39:24.270740: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
This API will be deprecated in the future versions. Please check the Github page for the current API

Modality facet for video 8qrpnFRGt2A segment 13 is (partially) missing and is thus being replaced by zeros!

Train on 1283 samples, validate on 229 samples
Epoch 1/100

  64/1283 [>.............................] - ETA: 4s - loss: 0.6881 - acc: 0.5312
 256/1283 [====>.........................] - ETA: 1s - loss: 0.6943 - acc: 0.5430
 384/1283 [=======>......................] - ETA: 0s - loss: 0.6908 - acc: 0.5495
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6898 - acc: 0.5371
 640/1283 [=============>................] - ETA: 0s - loss: 0.6885 - acc: 0.5359
 768/1283 [================>.............] - ETA: 0s - loss: 0.6872 - acc: 0.5469
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6860 - acc: 0.5536
1024/1283 [======================>.......] - ETA: 0s - loss: 0.6856 - acc: 0.5557
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6830 - acc: 0.5651
1280/1283 [============================>.] - ETA: 0s - loss: 0.6798 - acc: 0.5648
1283/1283 [==============================] - 1s 715us/step - loss: 0.6797 - acc: 0.5651 - val_loss: 0.6945 - val_acc: 0.5328

Epoch 00001: val_acc improved from -inf to 0.53275, saving model to classification_logs//lstm_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.1_nl_1_ml_20/saved_models/best_validation_lstm_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.1_nl_1_ml_20.ckpt
Epoch 2/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6577 - acc: 0.6719
 192/1283 [===>..........................] - ETA: 0s - loss: 0.6557 - acc: 0.6458
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6596 - acc: 0.6188
 448/1283 [=========>....................] - ETA: 0s - loss: 0.6624 - acc: 0.6049
 576/1283 [============>.................] - ETA: 0s - loss: 0.6625 - acc: 0.6111
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6644 - acc: 0.6023
 832/1283 [==================>...........] - ETA: 0s - loss: 0.6617 - acc: 0.6118
 960/1283 [=====================>........] - ETA: 0s - loss: 0.6603 - acc: 0.6104
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6624 - acc: 0.6039
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6615 - acc: 0.6050
1280/1283 [============================>.] - ETA: 0s - loss: 0.6626 - acc: 0.6023
1283/1283 [==============================] - 1s 603us/step - loss: 0.6630 - acc: 0.6009 - val_loss: 0.7000 - val_acc: 0.5328

Epoch 00002: val_acc did not improve
Epoch 3/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6722 - acc: 0.5469
 192/1283 [===>..........................] - ETA: 0s - loss: 0.6552 - acc: 0.5781
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6496 - acc: 0.5844
 448/1283 [=========>....................] - ETA: 0s - loss: 0.6483 - acc: 0.5871
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6446 - acc: 0.5977
 640/1283 [=============>................] - ETA: 0s - loss: 0.6447 - acc: 0.6109
 768/1283 [================>.............] - ETA: 0s - loss: 0.6454 - acc: 0.6120
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6455 - acc: 0.6127
1024/1283 [======================>.......] - ETA: 0s - loss: 0.6478 - acc: 0.6055
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6495 - acc: 0.6033
1280/1283 [============================>.] - ETA: 0s - loss: 0.6476 - acc: 0.6086
1283/1283 [==============================] - 1s 599us/step - loss: 0.6478 - acc: 0.6080 - val_loss: 0.7139 - val_acc: 0.4803

Epoch 00003: val_acc did not improve
Epoch 4/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6166 - acc: 0.6406
 192/1283 [===>..........................] - ETA: 0s - loss: 0.6227 - acc: 0.6302
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6303 - acc: 0.6250
 448/1283 [=========>....................] - ETA: 0s - loss: 0.6311 - acc: 0.6205
 576/1283 [============>.................] - ETA: 0s - loss: 0.6306 - acc: 0.6285
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6299 - acc: 0.6321
 768/1283 [================>.............] - ETA: 0s - loss: 0.6331 - acc: 0.6263
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6319 - acc: 0.6283
1024/1283 [======================>.......] - ETA: 0s - loss: 0.6365 - acc: 0.6230
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6384 - acc: 0.6241
1280/1283 [============================>.] - ETA: 0s - loss: 0.6384 - acc: 0.6188
1283/1283 [==============================] - 1s 655us/step - loss: 0.6379 - acc: 0.6196 - val_loss: 0.7303 - val_acc: 0.4934

Epoch 00004: val_acc did not improve
Epoch 5/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5994 - acc: 0.6719
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5859 - acc: 0.6667
 256/1283 [====>.........................] - ETA: 0s - loss: 0.5778 - acc: 0.6953
 384/1283 [=======>......................] - ETA: 0s - loss: 0.5923 - acc: 0.6953
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6294 - acc: 0.6484
 576/1283 [============>.................] - ETA: 0s - loss: 0.6389 - acc: 0.6354
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6327 - acc: 0.6435
 832/1283 [==================>...........] - ETA: 0s - loss: 0.6323 - acc: 0.6418
 960/1283 [=====================>........] - ETA: 0s - loss: 0.6319 - acc: 0.6427
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6316 - acc: 0.6443
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6350 - acc: 0.6398
1280/1283 [============================>.] - ETA: 0s - loss: 0.6356 - acc: 0.6359
1283/1283 [==============================] - 1s 766us/step - loss: 0.6362 - acc: 0.6352 - val_loss: 0.7255 - val_acc: 0.4891

Epoch 00005: val_acc did not improve
Epoch 6/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6292 - acc: 0.5625
 192/1283 [===>..........................] - ETA: 0s - loss: 0.6246 - acc: 0.5833
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6298 - acc: 0.5844
 448/1283 [=========>....................] - ETA: 0s - loss: 0.6247 - acc: 0.5960
 576/1283 [============>.................] - ETA: 0s - loss: 0.6264 - acc: 0.6076
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6329 - acc: 0.6065
 832/1283 [==================>...........] - ETA: 0s - loss: 0.6315 - acc: 0.6046
 960/1283 [=====================>........] - ETA: 0s - loss: 0.6304 - acc: 0.6167
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6285 - acc: 0.6167
1216/1283 [===========================>..] - ETA: 0s - loss: 0.6240 - acc: 0.6291
1280/1283 [============================>.] - ETA: 0s - loss: 0.6261 - acc: 0.6281
1283/1283 [==============================] - 1s 664us/step - loss: 0.6261 - acc: 0.6282 - val_loss: 0.7206 - val_acc: 0.5022

Epoch 00006: val_acc did not improve
Epoch 7/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5892 - acc: 0.6875
 192/1283 [===>..........................] - ETA: 0s - loss: 0.6178 - acc: 0.6354
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5998 - acc: 0.6656
 448/1283 [=========>....................] - ETA: 0s - loss: 0.6036 - acc: 0.6674
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6112 - acc: 0.6523
 640/1283 [=============>................] - ETA: 0s - loss: 0.6085 - acc: 0.6516
 768/1283 [================>.............] - ETA: 0s - loss: 0.6100 - acc: 0.6458
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6134 - acc: 0.6373
1024/1283 [======================>.......] - ETA: 0s - loss: 0.6140 - acc: 0.6387
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6154 - acc: 0.6354
1280/1283 [============================>.] - ETA: 0s - loss: 0.6185 - acc: 0.6312
1283/1283 [==============================] - 1s 684us/step - loss: 0.6180 - acc: 0.6321 - val_loss: 0.7288 - val_acc: 0.5066

Epoch 00007: val_acc did not improve
Epoch 8/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6108 - acc: 0.6250
 192/1283 [===>..........................] - ETA: 0s - loss: 0.6267 - acc: 0.6094
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6409 - acc: 0.5906
 448/1283 [=========>....................] - ETA: 0s - loss: 0.6125 - acc: 0.6138
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6096 - acc: 0.6230
 640/1283 [=============>................] - ETA: 0s - loss: 0.6113 - acc: 0.6266
 768/1283 [================>.............] - ETA: 0s - loss: 0.6153 - acc: 0.6185
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6128 - acc: 0.6295
1024/1283 [======================>.......] - ETA: 0s - loss: 0.6064 - acc: 0.6465
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6088 - acc: 0.6502
1280/1283 [============================>.] - ETA: 0s - loss: 0.6066 - acc: 0.6516
1283/1283 [==============================] - 1s 601us/step - loss: 0.6069 - acc: 0.6508 - val_loss: 0.7493 - val_acc: 0.4978

Epoch 00008: val_acc did not improve
Epoch 9/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6225 - acc: 0.6719
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5729 - acc: 0.7396
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5818 - acc: 0.7031
 448/1283 [=========>....................] - ETA: 0s - loss: 0.5841 - acc: 0.6897
 576/1283 [============>.................] - ETA: 0s - loss: 0.5786 - acc: 0.6892
 704/1283 [===============>..............] - ETA: 0s - loss: 0.5909 - acc: 0.6747
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5953 - acc: 0.6671
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5982 - acc: 0.6604
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6045 - acc: 0.6517
1216/1283 [===========================>..] - ETA: 0s - loss: 0.6056 - acc: 0.6497
1283/1283 [==============================] - 1s 568us/step - loss: 0.6048 - acc: 0.6500 - val_loss: 0.7752 - val_acc: 0.4410

Epoch 00009: val_acc did not improve
Epoch 10/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6184 - acc: 0.6719
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5903 - acc: 0.6823
 256/1283 [====>.........................] - ETA: 0s - loss: 0.6109 - acc: 0.6758
 384/1283 [=======>......................] - ETA: 0s - loss: 0.5946 - acc: 0.6875
 512/1283 [==========>...................] - ETA: 0s - loss: 0.5887 - acc: 0.6934
 640/1283 [=============>................] - ETA: 0s - loss: 0.5938 - acc: 0.6828
 704/1283 [===============>..............] - ETA: 0s - loss: 0.5922 - acc: 0.6832
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5981 - acc: 0.6755
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5983 - acc: 0.6740
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6041 - acc: 0.6682
1216/1283 [===========================>..] - ETA: 0s - loss: 0.6017 - acc: 0.6702
1283/1283 [==============================] - 1s 728us/step - loss: 0.6015 - acc: 0.6719 - val_loss: 0.7774 - val_acc: 0.4760

Epoch 00010: val_acc did not improve
Epoch 11/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5867 - acc: 0.7031
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5892 - acc: 0.6823
 256/1283 [====>.........................] - ETA: 0s - loss: 0.5662 - acc: 0.7031
 384/1283 [=======>......................] - ETA: 0s - loss: 0.5843 - acc: 0.6849
 512/1283 [==========>...................] - ETA: 0s - loss: 0.5727 - acc: 0.6895
 640/1283 [=============>................] - ETA: 0s - loss: 0.5762 - acc: 0.6859
 768/1283 [================>.............] - ETA: 0s - loss: 0.5842 - acc: 0.6810
 896/1283 [===================>..........] - ETA: 0s - loss: 0.5824 - acc: 0.6830
1024/1283 [======================>.......] - ETA: 0s - loss: 0.5851 - acc: 0.6807
1152/1283 [=========================>....] - ETA: 0s - loss: 0.5937 - acc: 0.6701
1280/1283 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.6633
1283/1283 [==============================] - 1s 701us/step - loss: 0.5952 - acc: 0.6641 - val_loss: 0.7914 - val_acc: 0.4279

Epoch 00011: val_acc did not improve
Epoch 00011: early stopping
batch_size=64
batch_norm=False
dropout_rate=0.1
n_layers=1
max_len=20
mode=V
accuracy=0.5379008746355685
best_valid_accuracy=0.5
