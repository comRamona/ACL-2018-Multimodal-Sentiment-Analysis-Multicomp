/afs/inf.ed.ac.uk/user/s17/s1738075/.local/lib/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
/afs/inf.ed.ac.uk/user/s17/s1738075/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
early_fusion_cnn.py:139: RuntimeWarning: invalid value encountered in divide
  train_set_audio = train_set_audio / audio_max
early_fusion_cnn.py:141: RuntimeWarning: invalid value encountered in divide
  test_set_audio = test_set_audio / audio_max
2018-03-20 23:15:24.229855: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
This API will be deprecated in the future versions. Please check the Github page for the current API

Modality facet for video 8qrpnFRGt2A segment 13 is (partially) missing and is thus being replaced by zeros!

Train on 1283 samples, validate on 229 samples
Epoch 1/100

  64/1283 [>.............................] - ETA: 3s - loss: 0.6968 - acc: 0.5000
 320/1283 [======>.......................] - ETA: 0s - loss: 0.7070 - acc: 0.5219
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6950 - acc: 0.5430
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6796 - acc: 0.5724
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6747 - acc: 0.5714
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6763 - acc: 0.5781
1283/1283 [==============================] - 1s 472us/step - loss: 0.6767 - acc: 0.5752 - val_loss: 0.7030 - val_acc: 0.5153

Epoch 00001: val_acc improved from -inf to 0.51528, saving model to classification_logs//cnn_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.2_nl_1_ml_30/saved_models/best_validation_cnn_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.2_nl_1_ml_30.ckpt
Epoch 2/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6415 - acc: 0.6406
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6510 - acc: 0.5875
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6612 - acc: 0.5742
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6545 - acc: 0.5994
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6437 - acc: 0.6272
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6609 - acc: 0.6259
1280/1283 [============================>.] - ETA: 0s - loss: 0.6563 - acc: 0.6289
1283/1283 [==============================] - 0s 297us/step - loss: 0.6562 - acc: 0.6290 - val_loss: 0.7116 - val_acc: 0.5415

Epoch 00002: val_acc improved from 0.51528 to 0.54148, saving model to classification_logs//cnn_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.2_nl_1_ml_30/saved_models/best_validation_cnn_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.2_nl_1_ml_30.ckpt
Epoch 3/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6151 - acc: 0.7031
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6166 - acc: 0.6562
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6112 - acc: 0.6621
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6202 - acc: 0.6449
 832/1283 [==================>...........] - ETA: 0s - loss: 0.6163 - acc: 0.6562
 960/1283 [=====================>........] - ETA: 0s - loss: 0.6272 - acc: 0.6385
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6425 - acc: 0.6354
1283/1283 [==============================] - 0s 379us/step - loss: 0.6366 - acc: 0.6407 - val_loss: 0.7406 - val_acc: 0.4847

Epoch 00003: val_acc did not improve
Epoch 4/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5685 - acc: 0.7188
 192/1283 [===>..........................] - ETA: 0s - loss: 0.6646 - acc: 0.6823
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6431 - acc: 0.6500
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6373 - acc: 0.6523
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6349 - acc: 0.6506
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6308 - acc: 0.6440
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6238 - acc: 0.6526
1280/1283 [============================>.] - ETA: 0s - loss: 0.6157 - acc: 0.6648
1283/1283 [==============================] - 1s 424us/step - loss: 0.6164 - acc: 0.6641 - val_loss: 0.7382 - val_acc: 0.5153

Epoch 00004: val_acc did not improve
Epoch 5/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6043 - acc: 0.6719
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5740 - acc: 0.7188
 256/1283 [====>.........................] - ETA: 0s - loss: 0.5783 - acc: 0.7070
 384/1283 [=======>......................] - ETA: 0s - loss: 0.5972 - acc: 0.6719
 512/1283 [==========>...................] - ETA: 0s - loss: 0.5933 - acc: 0.6836
 640/1283 [=============>................] - ETA: 0s - loss: 0.5985 - acc: 0.6750
 768/1283 [================>.............] - ETA: 0s - loss: 0.6038 - acc: 0.6628
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6006 - acc: 0.6629
1088/1283 [========================>.....] - ETA: 0s - loss: 0.5893 - acc: 0.6811
1280/1283 [============================>.] - ETA: 0s - loss: 0.5968 - acc: 0.6844
1283/1283 [==============================] - 1s 566us/step - loss: 0.5975 - acc: 0.6836 - val_loss: 0.7607 - val_acc: 0.4891

Epoch 00005: val_acc did not improve
Epoch 6/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5228 - acc: 0.6875
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5845 - acc: 0.6667
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5891 - acc: 0.6625
 448/1283 [=========>....................] - ETA: 0s - loss: 0.5919 - acc: 0.6652
 576/1283 [============>.................] - ETA: 0s - loss: 0.6110 - acc: 0.6840
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6029 - acc: 0.6903
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5960 - acc: 0.6935
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5919 - acc: 0.6896
1088/1283 [========================>.....] - ETA: 0s - loss: 0.5892 - acc: 0.6866
1216/1283 [===========================>..] - ETA: 0s - loss: 0.5887 - acc: 0.6842
1283/1283 [==============================] - 1s 622us/step - loss: 0.5869 - acc: 0.6859 - val_loss: 0.7946 - val_acc: 0.4978

Epoch 00006: val_acc did not improve
Epoch 7/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5515 - acc: 0.7031
 128/1283 [=>............................] - ETA: 0s - loss: 0.5655 - acc: 0.6875
 256/1283 [====>.........................] - ETA: 0s - loss: 0.5866 - acc: 0.6758
 384/1283 [=======>......................] - ETA: 0s - loss: 0.5639 - acc: 0.6875
 512/1283 [==========>...................] - ETA: 0s - loss: 0.5674 - acc: 0.6836
 640/1283 [=============>................] - ETA: 0s - loss: 0.5716 - acc: 0.6797
 768/1283 [================>.............] - ETA: 0s - loss: 0.5681 - acc: 0.6901
 896/1283 [===================>..........] - ETA: 0s - loss: 0.5783 - acc: 0.6964
1024/1283 [======================>.......] - ETA: 0s - loss: 0.5756 - acc: 0.6934
1152/1283 [=========================>....] - ETA: 0s - loss: 0.5748 - acc: 0.6953
1283/1283 [==============================] - 1s 528us/step - loss: 0.5724 - acc: 0.6968 - val_loss: 0.8479 - val_acc: 0.4585

Epoch 00007: val_acc did not improve
Epoch 8/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5245 - acc: 0.6875
 128/1283 [=>............................] - ETA: 0s - loss: 0.5249 - acc: 0.6875
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5313 - acc: 0.6667
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5239 - acc: 0.6875
 448/1283 [=========>....................] - ETA: 0s - loss: 0.5266 - acc: 0.6987
 576/1283 [============>.................] - ETA: 0s - loss: 0.5611 - acc: 0.6944
 704/1283 [===============>..............] - ETA: 0s - loss: 0.5620 - acc: 0.7003
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5526 - acc: 0.7115
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5514 - acc: 0.7135
1088/1283 [========================>.....] - ETA: 0s - loss: 0.5481 - acc: 0.7151
1216/1283 [===========================>..] - ETA: 0s - loss: 0.5464 - acc: 0.7204
1283/1283 [==============================] - 1s 668us/step - loss: 0.5504 - acc: 0.7186 - val_loss: 0.8462 - val_acc: 0.4847

Epoch 00008: val_acc did not improve
Epoch 9/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5412 - acc: 0.6719
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5460 - acc: 0.7083
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5226 - acc: 0.7375
 448/1283 [=========>....................] - ETA: 0s - loss: 0.5195 - acc: 0.7545
 576/1283 [============>.................] - ETA: 0s - loss: 0.5208 - acc: 0.7517
 704/1283 [===============>..............] - ETA: 0s - loss: 0.5161 - acc: 0.7571
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5184 - acc: 0.7488
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5214 - acc: 0.7438
1088/1283 [========================>.....] - ETA: 0s - loss: 0.5172 - acc: 0.7463
1216/1283 [===========================>..] - ETA: 0s - loss: 0.5240 - acc: 0.7500
1283/1283 [==============================] - 1s 531us/step - loss: 0.5244 - acc: 0.7475 - val_loss: 0.8429 - val_acc: 0.5284

Epoch 00009: val_acc did not improve
Epoch 10/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.4421 - acc: 0.8438
 192/1283 [===>..........................] - ETA: 0s - loss: 0.4748 - acc: 0.8073
 320/1283 [======>.......................] - ETA: 0s - loss: 0.4948 - acc: 0.7906
 448/1283 [=========>....................] - ETA: 0s - loss: 0.4892 - acc: 0.7857
 576/1283 [============>.................] - ETA: 0s - loss: 0.4866 - acc: 0.7795
 704/1283 [===============>..............] - ETA: 0s - loss: 0.4930 - acc: 0.7713
 832/1283 [==================>...........] - ETA: 0s - loss: 0.4997 - acc: 0.7680
 960/1283 [=====================>........] - ETA: 0s - loss: 0.4956 - acc: 0.7708
1088/1283 [========================>.....] - ETA: 0s - loss: 0.5108 - acc: 0.7693
1216/1283 [===========================>..] - ETA: 0s - loss: 0.5027 - acc: 0.7706
1283/1283 [==============================] - 1s 510us/step - loss: 0.5022 - acc: 0.7693 - val_loss: 0.8402 - val_acc: 0.5284

Epoch 00010: val_acc did not improve
Epoch 11/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.4610 - acc: 0.8438
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5358 - acc: 0.8021
 320/1283 [======>.......................] - ETA: 0s - loss: 0.4801 - acc: 0.8281
 448/1283 [=========>....................] - ETA: 0s - loss: 0.4803 - acc: 0.8103
 576/1283 [============>.................] - ETA: 0s - loss: 0.5048 - acc: 0.7847
 704/1283 [===============>..............] - ETA: 0s - loss: 0.4946 - acc: 0.7784
 832/1283 [==================>...........] - ETA: 0s - loss: 0.4855 - acc: 0.7788
 960/1283 [=====================>........] - ETA: 0s - loss: 0.4932 - acc: 0.7646
1088/1283 [========================>.....] - ETA: 0s - loss: 0.4942 - acc: 0.7638
1216/1283 [===========================>..] - ETA: 0s - loss: 0.5007 - acc: 0.7623
1283/1283 [==============================] - 1s 628us/step - loss: 0.5014 - acc: 0.7615 - val_loss: 0.8957 - val_acc: 0.5066

Epoch 00011: val_acc did not improve
Epoch 12/100

  64/1283 [>.............................] - ETA: 1s - loss: 0.6641 - acc: 0.6094
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5242 - acc: 0.7135
 320/1283 [======>.......................] - ETA: 0s - loss: 0.4881 - acc: 0.7312
 384/1283 [=======>......................] - ETA: 0s - loss: 0.4834 - acc: 0.7370
 448/1283 [=========>....................] - ETA: 0s - loss: 0.4844 - acc: 0.7433
 576/1283 [============>.................] - ETA: 0s - loss: 0.5028 - acc: 0.7292
 704/1283 [===============>..............] - ETA: 0s - loss: 0.4991 - acc: 0.7344
 832/1283 [==================>...........] - ETA: 0s - loss: 0.4945 - acc: 0.7452
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5094 - acc: 0.7490
1088/1283 [========================>.....] - ETA: 0s - loss: 0.5063 - acc: 0.7509
1216/1283 [===========================>..] - ETA: 0s - loss: 0.5087 - acc: 0.7434
1283/1283 [==============================] - 1s 712us/step - loss: 0.5038 - acc: 0.7467 - val_loss: 0.9228 - val_acc: 0.4672

Epoch 00012: val_acc did not improve
Epoch 00012: early stopping
batch_size=64
batch_norm=False
dropout_rate=0.2
n_layers=1
max_len=30
epochs=100
mode=V
accuracy=0.5510204081632653
best_valid_accuracy=0.5379008746355685
