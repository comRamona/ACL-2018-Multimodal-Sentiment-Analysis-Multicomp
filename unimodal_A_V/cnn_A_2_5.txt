/afs/inf.ed.ac.uk/user/s17/s1738075/.local/lib/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
/afs/inf.ed.ac.uk/user/s17/s1738075/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
early_fusion_cnn.py:139: RuntimeWarning: invalid value encountered in divide
  train_set_audio = train_set_audio / audio_max
early_fusion_cnn.py:141: RuntimeWarning: invalid value encountered in divide
  test_set_audio = test_set_audio / audio_max
2018-03-20 23:08:08.251844: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
This API will be deprecated in the future versions. Please check the Github page for the current API

Modality facet for video 8qrpnFRGt2A segment 13 is (partially) missing and is thus being replaced by zeros!

Train on 1283 samples, validate on 229 samples
Epoch 1/100

  64/1283 [>.............................] - ETA: 4s - loss: 0.6796 - acc: 0.5938
 256/1283 [====>.........................] - ETA: 1s - loss: 0.7390 - acc: 0.5352
 448/1283 [=========>....................] - ETA: 0s - loss: 0.7375 - acc: 0.5156
 704/1283 [===============>..............] - ETA: 0s - loss: 0.7273 - acc: 0.5241
 960/1283 [=====================>........] - ETA: 0s - loss: 0.7207 - acc: 0.5323
1216/1283 [===========================>..] - ETA: 0s - loss: 0.7126 - acc: 0.5411
1283/1283 [==============================] - 1s 545us/step - loss: 0.7112 - acc: 0.5425 - val_loss: 0.7134 - val_acc: 0.4978

Epoch 00001: val_acc improved from -inf to 0.49782, saving model to classification_logs//cnn_early_fusion_m_A_ep_100_bs_64_bn_False_dr_0.2_nl_2_ml_15/saved_models/best_validation_cnn_early_fusion_m_A_ep_100_bs_64_bn_False_dr_0.2_nl_2_ml_15.ckpt
Epoch 2/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6416 - acc: 0.6875
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6467 - acc: 0.6156
 576/1283 [============>.................] - ETA: 0s - loss: 0.6413 - acc: 0.6267
 832/1283 [==================>...........] - ETA: 0s - loss: 0.6289 - acc: 0.6394
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6296 - acc: 0.6471
1280/1283 [============================>.] - ETA: 0s - loss: 0.6284 - acc: 0.6445
1283/1283 [==============================] - 0s 272us/step - loss: 0.6292 - acc: 0.6438 - val_loss: 0.7231 - val_acc: 0.5502

Epoch 00002: val_acc improved from 0.49782 to 0.55022, saving model to classification_logs//cnn_early_fusion_m_A_ep_100_bs_64_bn_False_dr_0.2_nl_2_ml_15/saved_models/best_validation_cnn_early_fusion_m_A_ep_100_bs_64_bn_False_dr_0.2_nl_2_ml_15.ckpt
Epoch 3/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5783 - acc: 0.6562
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5954 - acc: 0.6781
 512/1283 [==========>...................] - ETA: 0s - loss: 0.5972 - acc: 0.6914
 704/1283 [===============>..............] - ETA: 0s - loss: 0.5898 - acc: 0.7060
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5862 - acc: 0.7135
1152/1283 [=========================>....] - ETA: 0s - loss: 0.5868 - acc: 0.7153
1280/1283 [============================>.] - ETA: 0s - loss: 0.5837 - acc: 0.7164
1283/1283 [==============================] - 0s 313us/step - loss: 0.5835 - acc: 0.7163 - val_loss: 0.7309 - val_acc: 0.5459

Epoch 00003: val_acc did not improve
Epoch 4/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.4555 - acc: 0.9062
 256/1283 [====>.........................] - ETA: 0s - loss: 0.5015 - acc: 0.8008
 384/1283 [=======>......................] - ETA: 0s - loss: 0.5070 - acc: 0.8073
 640/1283 [=============>................] - ETA: 0s - loss: 0.5160 - acc: 0.7875
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5196 - acc: 0.7728
1024/1283 [======================>.......] - ETA: 0s - loss: 0.5177 - acc: 0.7734
1280/1283 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.7656
1283/1283 [==============================] - 0s 313us/step - loss: 0.5184 - acc: 0.7654 - val_loss: 0.8165 - val_acc: 0.5721

Epoch 00004: val_acc improved from 0.55022 to 0.57205, saving model to classification_logs//cnn_early_fusion_m_A_ep_100_bs_64_bn_False_dr_0.2_nl_2_ml_15/saved_models/best_validation_cnn_early_fusion_m_A_ep_100_bs_64_bn_False_dr_0.2_nl_2_ml_15.ckpt
Epoch 5/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.4250 - acc: 0.7969
 256/1283 [====>.........................] - ETA: 0s - loss: 0.4567 - acc: 0.7969
 384/1283 [=======>......................] - ETA: 0s - loss: 0.4648 - acc: 0.7812
 576/1283 [============>.................] - ETA: 0s - loss: 0.4463 - acc: 0.8003
 768/1283 [================>.............] - ETA: 0s - loss: 0.4638 - acc: 0.7865
 960/1283 [=====================>........] - ETA: 0s - loss: 0.4557 - acc: 0.7896
1088/1283 [========================>.....] - ETA: 0s - loss: 0.4571 - acc: 0.7831
1216/1283 [===========================>..] - ETA: 0s - loss: 0.4626 - acc: 0.7812
1283/1283 [==============================] - 1s 417us/step - loss: 0.4623 - acc: 0.7818 - val_loss: 0.7941 - val_acc: 0.5808

Epoch 00005: val_acc improved from 0.57205 to 0.58079, saving model to classification_logs//cnn_early_fusion_m_A_ep_100_bs_64_bn_False_dr_0.2_nl_2_ml_15/saved_models/best_validation_cnn_early_fusion_m_A_ep_100_bs_64_bn_False_dr_0.2_nl_2_ml_15.ckpt
Epoch 6/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.4337 - acc: 0.7812
 256/1283 [====>.........................] - ETA: 0s - loss: 0.3936 - acc: 0.8125
 448/1283 [=========>....................] - ETA: 0s - loss: 0.3819 - acc: 0.8326
 640/1283 [=============>................] - ETA: 0s - loss: 0.3819 - acc: 0.8266
 896/1283 [===================>..........] - ETA: 0s - loss: 0.3713 - acc: 0.8382
1024/1283 [======================>.......] - ETA: 0s - loss: 0.3728 - acc: 0.8350
1152/1283 [=========================>....] - ETA: 0s - loss: 0.3712 - acc: 0.8359
1283/1283 [==============================] - 0s 368us/step - loss: 0.3738 - acc: 0.8332 - val_loss: 0.8278 - val_acc: 0.5983

Epoch 00006: val_acc improved from 0.58079 to 0.59825, saving model to classification_logs//cnn_early_fusion_m_A_ep_100_bs_64_bn_False_dr_0.2_nl_2_ml_15/saved_models/best_validation_cnn_early_fusion_m_A_ep_100_bs_64_bn_False_dr_0.2_nl_2_ml_15.ckpt
Epoch 7/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.2876 - acc: 0.9219
 256/1283 [====>.........................] - ETA: 0s - loss: 0.3264 - acc: 0.8672
 448/1283 [=========>....................] - ETA: 0s - loss: 0.3267 - acc: 0.8661
 576/1283 [============>.................] - ETA: 0s - loss: 0.3277 - acc: 0.8628
 768/1283 [================>.............] - ETA: 0s - loss: 0.3299 - acc: 0.8607
 896/1283 [===================>..........] - ETA: 0s - loss: 0.3197 - acc: 0.8694
1088/1283 [========================>.....] - ETA: 0s - loss: 0.3198 - acc: 0.8741
1216/1283 [===========================>..] - ETA: 0s - loss: 0.3139 - acc: 0.8758
1283/1283 [==============================] - 1s 456us/step - loss: 0.3106 - acc: 0.8792 - val_loss: 0.8706 - val_acc: 0.5371

Epoch 00007: val_acc did not improve
Epoch 8/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.2775 - acc: 0.8594
 192/1283 [===>..........................] - ETA: 0s - loss: 0.2390 - acc: 0.9219
 320/1283 [======>.......................] - ETA: 0s - loss: 0.2580 - acc: 0.9125
 448/1283 [=========>....................] - ETA: 0s - loss: 0.2454 - acc: 0.9263
 576/1283 [============>.................] - ETA: 0s - loss: 0.2461 - acc: 0.9236
 704/1283 [===============>..............] - ETA: 0s - loss: 0.2416 - acc: 0.9205
 832/1283 [==================>...........] - ETA: 0s - loss: 0.2397 - acc: 0.9231
1024/1283 [======================>.......] - ETA: 0s - loss: 0.2366 - acc: 0.9199
1216/1283 [===========================>..] - ETA: 0s - loss: 0.2354 - acc: 0.9202
1283/1283 [==============================] - 1s 438us/step - loss: 0.2342 - acc: 0.9213 - val_loss: 1.0468 - val_acc: 0.5197

Epoch 00008: val_acc did not improve
Epoch 9/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.1288 - acc: 1.0000
 192/1283 [===>..........................] - ETA: 0s - loss: 0.1421 - acc: 0.9844
 384/1283 [=======>......................] - ETA: 0s - loss: 0.1661 - acc: 0.9635
 576/1283 [============>.................] - ETA: 0s - loss: 0.1611 - acc: 0.9635
 704/1283 [===============>..............] - ETA: 0s - loss: 0.1644 - acc: 0.9545
 896/1283 [===================>..........] - ETA: 0s - loss: 0.1601 - acc: 0.9520
1088/1283 [========================>.....] - ETA: 0s - loss: 0.1591 - acc: 0.9540
1216/1283 [===========================>..] - ETA: 0s - loss: 0.1581 - acc: 0.9531
1283/1283 [==============================] - 0s 380us/step - loss: 0.1608 - acc: 0.9517 - val_loss: 1.4661 - val_acc: 0.5721

Epoch 00009: val_acc did not improve
Epoch 10/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.1495 - acc: 0.9375
 192/1283 [===>..........................] - ETA: 0s - loss: 0.3386 - acc: 0.8490
 320/1283 [======>.......................] - ETA: 0s - loss: 0.3028 - acc: 0.8656
 448/1283 [=========>....................] - ETA: 0s - loss: 0.3458 - acc: 0.8393
 640/1283 [=============>................] - ETA: 0s - loss: 0.3367 - acc: 0.8375
 768/1283 [================>.............] - ETA: 0s - loss: 0.3449 - acc: 0.8307
 896/1283 [===================>..........] - ETA: 0s - loss: 0.3202 - acc: 0.8471
1024/1283 [======================>.......] - ETA: 0s - loss: 0.3218 - acc: 0.8467
1152/1283 [=========================>....] - ETA: 0s - loss: 0.3127 - acc: 0.8533
1280/1283 [============================>.] - ETA: 0s - loss: 0.3003 - acc: 0.8625
1283/1283 [==============================] - 1s 493us/step - loss: 0.3000 - acc: 0.8628 - val_loss: 1.3146 - val_acc: 0.5677

Epoch 00010: val_acc did not improve
Epoch 11/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.3764 - acc: 0.7812
 192/1283 [===>..........................] - ETA: 0s - loss: 0.2391 - acc: 0.8750
 320/1283 [======>.......................] - ETA: 0s - loss: 0.2206 - acc: 0.9062
 448/1283 [=========>....................] - ETA: 0s - loss: 0.1970 - acc: 0.9196
 640/1283 [=============>................] - ETA: 0s - loss: 0.1943 - acc: 0.9250
 768/1283 [================>.............] - ETA: 0s - loss: 0.1891 - acc: 0.9323
 896/1283 [===================>..........] - ETA: 0s - loss: 0.1821 - acc: 0.9375
1024/1283 [======================>.......] - ETA: 0s - loss: 0.1791 - acc: 0.9414
1152/1283 [=========================>....] - ETA: 0s - loss: 0.1754 - acc: 0.9427
1283/1283 [==============================] - 1s 516us/step - loss: 0.1695 - acc: 0.9454 - val_loss: 1.1785 - val_acc: 0.5284

Epoch 00011: val_acc did not improve
Epoch 12/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.1082 - acc: 0.9844
 128/1283 [=>............................] - ETA: 0s - loss: 0.1002 - acc: 0.9766
 320/1283 [======>.......................] - ETA: 0s - loss: 0.1116 - acc: 0.9688
 512/1283 [==========>...................] - ETA: 0s - loss: 0.0980 - acc: 0.9785
 640/1283 [=============>................] - ETA: 0s - loss: 0.0945 - acc: 0.9812
 768/1283 [================>.............] - ETA: 0s - loss: 0.0926 - acc: 0.9818
 896/1283 [===================>..........] - ETA: 0s - loss: 0.0925 - acc: 0.9810
1088/1283 [========================>.....] - ETA: 0s - loss: 0.0879 - acc: 0.9825
1216/1283 [===========================>..] - ETA: 0s - loss: 0.0913 - acc: 0.9794
1283/1283 [==============================] - 1s 529us/step - loss: 0.0897 - acc: 0.9797 - val_loss: 1.2938 - val_acc: 0.5153

Epoch 00012: val_acc did not improve
Epoch 13/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.0753 - acc: 0.9844
 256/1283 [====>.........................] - ETA: 0s - loss: 0.0652 - acc: 0.9883
 448/1283 [=========>....................] - ETA: 0s - loss: 0.0596 - acc: 0.9888
 640/1283 [=============>................] - ETA: 0s - loss: 0.0568 - acc: 0.9875
 768/1283 [================>.............] - ETA: 0s - loss: 0.0593 - acc: 0.9857
 896/1283 [===================>..........] - ETA: 0s - loss: 0.0556 - acc: 0.9877
1024/1283 [======================>.......] - ETA: 0s - loss: 0.0585 - acc: 0.9873
1152/1283 [=========================>....] - ETA: 0s - loss: 0.0563 - acc: 0.9878
1280/1283 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9859
1283/1283 [==============================] - 1s 453us/step - loss: 0.0581 - acc: 0.9860 - val_loss: 1.5504 - val_acc: 0.5546

Epoch 00013: val_acc did not improve
Epoch 14/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.0376 - acc: 1.0000
 256/1283 [====>.........................] - ETA: 0s - loss: 0.0711 - acc: 0.9727
 448/1283 [=========>....................] - ETA: 0s - loss: 0.0679 - acc: 0.9754
 640/1283 [=============>................] - ETA: 0s - loss: 0.0709 - acc: 0.9797
 832/1283 [==================>...........] - ETA: 0s - loss: 0.0651 - acc: 0.9808
1024/1283 [======================>.......] - ETA: 0s - loss: 0.0628 - acc: 0.9824
1216/1283 [===========================>..] - ETA: 0s - loss: 0.0638 - acc: 0.9827
1283/1283 [==============================] - 0s 377us/step - loss: 0.0638 - acc: 0.9829 - val_loss: 1.5061 - val_acc: 0.5153

Epoch 00014: val_acc did not improve
Epoch 15/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.0558 - acc: 1.0000
 256/1283 [====>.........................] - ETA: 0s - loss: 0.1648 - acc: 0.9336
 448/1283 [=========>....................] - ETA: 0s - loss: 0.1425 - acc: 0.9442
 576/1283 [============>.................] - ETA: 0s - loss: 0.1396 - acc: 0.9427
 704/1283 [===============>..............] - ETA: 0s - loss: 0.1268 - acc: 0.9489
 896/1283 [===================>..........] - ETA: 0s - loss: 0.1319 - acc: 0.9464
1088/1283 [========================>.....] - ETA: 0s - loss: 0.1346 - acc: 0.9467
1280/1283 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9477
1283/1283 [==============================] - 0s 387us/step - loss: 0.1314 - acc: 0.9478 - val_loss: 1.4686 - val_acc: 0.5633

Epoch 00015: val_acc did not improve
Epoch 16/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.0591 - acc: 0.9688
 256/1283 [====>.........................] - ETA: 0s - loss: 0.0748 - acc: 0.9805
 384/1283 [=======>......................] - ETA: 0s - loss: 0.0734 - acc: 0.9792
 512/1283 [==========>...................] - ETA: 0s - loss: 0.0712 - acc: 0.9785
 704/1283 [===============>..............] - ETA: 0s - loss: 0.0808 - acc: 0.9702
 832/1283 [==================>...........] - ETA: 0s - loss: 0.0752 - acc: 0.9736
1024/1283 [======================>.......] - ETA: 0s - loss: 0.0718 - acc: 0.9756
1216/1283 [===========================>..] - ETA: 0s - loss: 0.0722 - acc: 0.9762
1283/1283 [==============================] - 1s 456us/step - loss: 0.0727 - acc: 0.9758 - val_loss: 1.4809 - val_acc: 0.5109

Epoch 00016: val_acc did not improve
Epoch 00016: early stopping
batch_size=64
batch_norm=False
dropout_rate=0.2
n_layers=2
max_len=15
epochs=100
mode=A
accuracy=0.49854227405247814
best_valid_accuracy=0.5
