/afs/inf.ed.ac.uk/user/s17/s1738075/.local/lib/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
/afs/inf.ed.ac.uk/user/s17/s1738075/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
early_fusion_cnn.py:139: RuntimeWarning: invalid value encountered in divide
  train_set_audio = train_set_audio / audio_max
early_fusion_cnn.py:141: RuntimeWarning: invalid value encountered in divide
  test_set_audio = test_set_audio / audio_max
2018-03-20 23:15:27.827920: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
This API will be deprecated in the future versions. Please check the Github page for the current API

Modality facet for video 8qrpnFRGt2A segment 13 is (partially) missing and is thus being replaced by zeros!

Train on 1283 samples, validate on 229 samples
Epoch 1/100

  64/1283 [>.............................] - ETA: 8s - loss: 0.6709 - acc: 0.6250
 256/1283 [====>.........................] - ETA: 2s - loss: 0.6787 - acc: 0.6055
 384/1283 [=======>......................] - ETA: 1s - loss: 0.6814 - acc: 0.5938
 448/1283 [=========>....................] - ETA: 1s - loss: 0.6800 - acc: 0.5871
 576/1283 [============>.................] - ETA: 0s - loss: 0.6725 - acc: 0.5747
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6763 - acc: 0.5724
 832/1283 [==================>...........] - ETA: 0s - loss: 0.6774 - acc: 0.5649
 960/1283 [=====================>........] - ETA: 0s - loss: 0.6765 - acc: 0.5646
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6915 - acc: 0.5708
1216/1283 [===========================>..] - ETA: 0s - loss: 0.6871 - acc: 0.5757
1283/1283 [==============================] - 1s 961us/step - loss: 0.6848 - acc: 0.5791 - val_loss: 0.7089 - val_acc: 0.5502

Epoch 00001: val_acc improved from -inf to 0.55022, saving model to classification_logs//cnn_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.1_nl_1_ml_25/saved_models/best_validation_cnn_early_fusion_m_V_ep_100_bs_64_bn_False_dr_0.1_nl_1_ml_25.ckpt
Epoch 2/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6248 - acc: 0.5938
 256/1283 [====>.........................] - ETA: 0s - loss: 0.6324 - acc: 0.6250
 384/1283 [=======>......................] - ETA: 0s - loss: 0.6326 - acc: 0.6146
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6767 - acc: 0.6016
 640/1283 [=============>................] - ETA: 0s - loss: 0.6704 - acc: 0.6141
 768/1283 [================>.............] - ETA: 0s - loss: 0.6636 - acc: 0.6120
 896/1283 [===================>..........] - ETA: 0s - loss: 0.6624 - acc: 0.6150
1024/1283 [======================>.......] - ETA: 0s - loss: 0.6520 - acc: 0.6270
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6518 - acc: 0.6232
1216/1283 [===========================>..] - ETA: 0s - loss: 0.6518 - acc: 0.6176
1280/1283 [============================>.] - ETA: 0s - loss: 0.6490 - acc: 0.6219
1283/1283 [==============================] - 1s 602us/step - loss: 0.6481 - acc: 0.6228 - val_loss: 0.7438 - val_acc: 0.5328

Epoch 00002: val_acc did not improve
Epoch 3/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.6118 - acc: 0.6250
 192/1283 [===>..........................] - ETA: 0s - loss: 0.6326 - acc: 0.5833
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6254 - acc: 0.6094
 448/1283 [=========>....................] - ETA: 0s - loss: 0.6273 - acc: 0.6116
 576/1283 [============>.................] - ETA: 0s - loss: 0.6193 - acc: 0.6302
 704/1283 [===============>..............] - ETA: 0s - loss: 0.6406 - acc: 0.6364
 832/1283 [==================>...........] - ETA: 0s - loss: 0.6293 - acc: 0.6454
1024/1283 [======================>.......] - ETA: 0s - loss: 0.6271 - acc: 0.6396
1152/1283 [=========================>....] - ETA: 0s - loss: 0.6284 - acc: 0.6458
1283/1283 [==============================] - 1s 552us/step - loss: 0.6292 - acc: 0.6391 - val_loss: 0.7562 - val_acc: 0.5022

Epoch 00003: val_acc did not improve
Epoch 4/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5580 - acc: 0.6875
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5994 - acc: 0.6979
 320/1283 [======>.......................] - ETA: 0s - loss: 0.6188 - acc: 0.6750
 384/1283 [=======>......................] - ETA: 0s - loss: 0.6148 - acc: 0.6693
 512/1283 [==========>...................] - ETA: 0s - loss: 0.6177 - acc: 0.6621
 640/1283 [=============>................] - ETA: 0s - loss: 0.6156 - acc: 0.6656
 768/1283 [================>.............] - ETA: 0s - loss: 0.6095 - acc: 0.6680
 832/1283 [==================>...........] - ETA: 0s - loss: 0.6091 - acc: 0.6647
 960/1283 [=====================>........] - ETA: 0s - loss: 0.6106 - acc: 0.6646
1088/1283 [========================>.....] - ETA: 0s - loss: 0.6069 - acc: 0.6627
1216/1283 [===========================>..] - ETA: 0s - loss: 0.6319 - acc: 0.6505
1283/1283 [==============================] - 1s 565us/step - loss: 0.6309 - acc: 0.6485 - val_loss: 0.7762 - val_acc: 0.4760

Epoch 00004: val_acc did not improve
Epoch 5/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5772 - acc: 0.7188
 192/1283 [===>..........................] - ETA: 0s - loss: 0.5767 - acc: 0.6667
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5973 - acc: 0.6438
 512/1283 [==========>...................] - ETA: 0s - loss: 0.5977 - acc: 0.6602
 640/1283 [=============>................] - ETA: 0s - loss: 0.5891 - acc: 0.6672
 768/1283 [================>.............] - ETA: 0s - loss: 0.5920 - acc: 0.6654
 896/1283 [===================>..........] - ETA: 0s - loss: 0.5887 - acc: 0.6763
1024/1283 [======================>.......] - ETA: 0s - loss: 0.5840 - acc: 0.6816
1216/1283 [===========================>..] - ETA: 0s - loss: 0.5966 - acc: 0.6809
1283/1283 [==============================] - 1s 464us/step - loss: 0.5974 - acc: 0.6797 - val_loss: 0.8316 - val_acc: 0.4672

Epoch 00005: val_acc did not improve
Epoch 6/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5488 - acc: 0.7188
 256/1283 [====>.........................] - ETA: 0s - loss: 0.5710 - acc: 0.6758
 448/1283 [=========>....................] - ETA: 0s - loss: 0.5644 - acc: 0.6808
 704/1283 [===============>..............] - ETA: 0s - loss: 0.5840 - acc: 0.6676
 896/1283 [===================>..........] - ETA: 0s - loss: 0.5835 - acc: 0.6942
1152/1283 [=========================>....] - ETA: 0s - loss: 0.5821 - acc: 0.6918
1283/1283 [==============================] - 0s 318us/step - loss: 0.5815 - acc: 0.6906 - val_loss: 0.8311 - val_acc: 0.4803

Epoch 00006: val_acc did not improve
Epoch 7/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5528 - acc: 0.7500
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5203 - acc: 0.7312
 576/1283 [============>.................] - ETA: 0s - loss: 0.5395 - acc: 0.7552
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5471 - acc: 0.7380
1088/1283 [========================>.....] - ETA: 0s - loss: 0.5678 - acc: 0.7132
1280/1283 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.7141
1283/1283 [==============================] - 0s 269us/step - loss: 0.5664 - acc: 0.7140 - val_loss: 0.7788 - val_acc: 0.5109

Epoch 00007: val_acc did not improve
Epoch 8/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5326 - acc: 0.7656
 320/1283 [======>.......................] - ETA: 0s - loss: 0.5904 - acc: 0.7188
 576/1283 [============>.................] - ETA: 0s - loss: 0.5689 - acc: 0.7257
 832/1283 [==================>...........] - ETA: 0s - loss: 0.5655 - acc: 0.7272
1088/1283 [========================>.....] - ETA: 0s - loss: 0.5557 - acc: 0.7307
1280/1283 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.7250
1283/1283 [==============================] - 0s 253us/step - loss: 0.5536 - acc: 0.7256 - val_loss: 0.7678 - val_acc: 0.5197

Epoch 00008: val_acc did not improve
Epoch 9/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.5397 - acc: 0.7031
 256/1283 [====>.........................] - ETA: 0s - loss: 0.5389 - acc: 0.7266
 512/1283 [==========>...................] - ETA: 0s - loss: 0.5235 - acc: 0.7344
 704/1283 [===============>..............] - ETA: 0s - loss: 0.5146 - acc: 0.7386
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5175 - acc: 0.7323
1152/1283 [=========================>....] - ETA: 0s - loss: 0.5180 - acc: 0.7344
1283/1283 [==============================] - 0s 288us/step - loss: 0.5275 - acc: 0.7405 - val_loss: 0.8010 - val_acc: 0.4541

Epoch 00009: val_acc did not improve
Epoch 10/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.4916 - acc: 0.7656
 256/1283 [====>.........................] - ETA: 0s - loss: 0.5516 - acc: 0.7539
 448/1283 [=========>....................] - ETA: 0s - loss: 0.5393 - acc: 0.7589
 704/1283 [===============>..............] - ETA: 0s - loss: 0.5419 - acc: 0.7443
 896/1283 [===================>..........] - ETA: 0s - loss: 0.5261 - acc: 0.7467
1088/1283 [========================>.....] - ETA: 0s - loss: 0.5182 - acc: 0.7482
1283/1283 [==============================] - 0s 297us/step - loss: 0.5155 - acc: 0.7467 - val_loss: 0.8373 - val_acc: 0.4978

Epoch 00010: val_acc did not improve
Epoch 11/100

  64/1283 [>.............................] - ETA: 0s - loss: 0.4531 - acc: 0.8281
 320/1283 [======>.......................] - ETA: 0s - loss: 0.4964 - acc: 0.7562
 640/1283 [=============>................] - ETA: 0s - loss: 0.5075 - acc: 0.7516
 960/1283 [=====================>........] - ETA: 0s - loss: 0.5115 - acc: 0.7448
1280/1283 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.7344
1283/1283 [==============================] - 0s 202us/step - loss: 0.5218 - acc: 0.7350 - val_loss: 0.8831 - val_acc: 0.4760

Epoch 00011: val_acc did not improve
Epoch 00011: early stopping
batch_size=64
batch_norm=False
dropout_rate=0.1
n_layers=1
max_len=25
epochs=100
mode=V
accuracy=0.5787172011661808
best_valid_accuracy=0.48104956268221577
